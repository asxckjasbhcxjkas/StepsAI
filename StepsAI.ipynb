{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "koaNo_HsSopF",
      "metadata": {
        "id": "koaNo_HsSopF"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt #download from github and upload it here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b54a848-6acc-47d7-9715-ec15aa704d00",
      "metadata": {
        "id": "3b54a848-6acc-47d7-9715-ec15aa704d00"
      },
      "source": [
        "# Cuda Docs Crawling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1B6MbjkrTHeP",
      "metadata": {
        "id": "1B6MbjkrTHeP"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61fe6b9-5445-4d08-ae5a-e80da431e51d",
      "metadata": {
        "id": "c61fe6b9-5445-4d08-ae5a-e80da431e51d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "visited_pages = set()\n",
        "gathered_content = []\n",
        "semaphore = asyncio.Semaphore(10)  # Limit concurrent requests\n",
        "\n",
        "async def fetch_page_content(session, target_url, level):\n",
        "    if level > 5 or target_url in visited_pages: #change 5 to 1 for running purpose otherwise it will take forever to crawl lakhs of links\n",
        "        return\n",
        "    visited_pages.add(target_url)\n",
        "\n",
        "    try:\n",
        "        async with semaphore:\n",
        "            async with session.get(target_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    print(f\"Failed to access {target_url}: HTTP {response.status}\")\n",
        "                    return\n",
        "\n",
        "                content = await response.text()\n",
        "                page_soup = BeautifulSoup(content, 'lxml')\n",
        "                print(f\"Processing: {target_url}\")\n",
        "\n",
        "                page_text = page_soup.get_text(separator=\"\\n\", strip=True)\n",
        "                gathered_content.append(f\"Source: {target_url}\\n\\n{page_text[:750]}\\n{'='*50}\\n\")\n",
        "\n",
        "                tasks = []\n",
        "                for anchor in page_soup.find_all('a', href=True):\n",
        "                    child_url = urljoin(target_url, anchor['href'])\n",
        "                    parsed_child = urlparse(child_url)\n",
        "                    if (parsed_child.netloc == urlparse(target_url).netloc and\n",
        "                        parsed_child.scheme in [\"http\", \"https\"]):\n",
        "                        tasks.append(asyncio.ensure_future(\n",
        "                            fetch_page_content(session, child_url, level + 1)\n",
        "                        ))\n",
        "\n",
        "                await asyncio.gather(*tasks)\n",
        "\n",
        "    except asyncio.TimeoutError:\n",
        "        print(f\"Timeout while accessing {target_url}\")\n",
        "    except Exception as general_err:\n",
        "        print(f\"Unexpected error while processing {target_url}: {general_err}\")\n",
        "\n",
        "async def main():\n",
        "    initial_url = \"https://docs.nvidia.com/cuda/\"\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        await fetch_page_content(session, initial_url, 0)\n",
        "\n",
        "    output_file = 'gathered_web_content.txt'\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        outfile.writelines(gathered_content)\n",
        "    print(f'Content gathering completed. Data saved to {output_file}')\n",
        "\n",
        "def run_async_scraper():\n",
        "    \"\"\"Run the scraper in a new event loop.\"\"\"\n",
        "\n",
        "    asyncio.run(main())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  await main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f1738f3-3398-4b9b-8dbb-25335efe70cc",
      "metadata": {
        "id": "3f1738f3-3398-4b9b-8dbb-25335efe70cc"
      },
      "source": [
        "# Data Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211ec630-f627-4f9a-828d-41c4447393dd",
      "metadata": {
        "id": "211ec630-f627-4f9a-828d-41c4447393dd"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Initialize NLP components\n",
        "print(\"Loading NLP models...\")\n",
        "text_processor = spacy.load(\"en_core_web_trf\")\n",
        "text_processor.max_length = 1000000\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"NLP models loaded successfully.\")\n",
        "\n",
        "def import_content(filepath):\n",
        "    print(f\"Importing content from {filepath}...\")\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        raw_content = file.read()\n",
        "\n",
        "    # Split the content by the separator used in the original file\n",
        "    content_blocks = raw_content.split(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "    processed_content = []\n",
        "\n",
        "    for block in content_blocks:\n",
        "        if block.strip():\n",
        "            components = block.split(\"\\n\", 1)\n",
        "            source = components[0].replace(\"Source: \", \"\")\n",
        "            body = components[1] if len(components) > 1 else \"\"\n",
        "            processed_content.append({'source': source, 'body': body})\n",
        "\n",
        "    print(f\"Imported {len(processed_content)} content blocks.\")\n",
        "    return processed_content\n",
        "\n",
        "def split_large_text(text, chunk_size=1500000):\n",
        "    \"\"\"Divide text into smaller portions of specified maximum size.\"\"\"\n",
        "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "def segment_content(raw_data):\n",
        "    print(\"Segmenting content...\")\n",
        "    segmented_data = []\n",
        "\n",
        "    for idx, content_item in enumerate(raw_data, 1):\n",
        "        segments = []\n",
        "        body = content_item.get('body', '')\n",
        "\n",
        "        # Handle large texts by splitting them\n",
        "        text_portions = split_large_text(body, chunk_size=1500000)\n",
        "\n",
        "        for portion in text_portions:\n",
        "            # Process text using spaCy for sentence detection\n",
        "            doc = text_processor(portion)\n",
        "\n",
        "            # Segment sentences based on semantic relatedness\n",
        "            current_segment = []\n",
        "            for sentence in doc.sents:\n",
        "                if current_segment:\n",
        "                    # Evaluate semantic relatedness between current segment and new sentence\n",
        "                    segment_vector = embedding_model.encode(\" \".join([str(s) for s in current_segment]))\n",
        "                    sentence_vector = embedding_model.encode(sentence.text)\n",
        "                    relatedness = cosine_similarity([segment_vector], [sentence_vector])[0][0]\n",
        "\n",
        "                    if relatedness < 0.75:  # Threshold for semantic relatedness\n",
        "                        segments.append(\" \".join([str(s) for s in current_segment]))\n",
        "                        current_segment = []\n",
        "\n",
        "                current_segment.append(sentence)\n",
        "\n",
        "            if current_segment:\n",
        "                segments.append(\" \".join([str(s) for s in current_segment]))\n",
        "\n",
        "        content_item['segments'] = segments\n",
        "        segmented_data.append(content_item)\n",
        "        print(f\"Processed item {idx}/{len(raw_data)}: {len(segments)} segments created.\")\n",
        "\n",
        "    return segmented_data\n",
        "\n",
        "def export_segmented_content(segmented_data, output_path):\n",
        "    print(f\"Exporting segmented content to {output_path}...\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        for content_item in segmented_data:\n",
        "            file.write(f\"Source: {content_item['source']}\\n\\n\")\n",
        "            for segment in content_item['segments']:\n",
        "                file.write(segment + '\\n\\n')\n",
        "            file.write(\"=\"*50 + '\\n\\n')\n",
        "    print(\"Export completed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    source_file = 'gathered_web_content.txt'\n",
        "    output_file = 'segmented_content.txt'\n",
        "\n",
        "    # Import gathered content\n",
        "    raw_data = import_content(source_file)\n",
        "\n",
        "    # Segment the content based on semantic relatedness\n",
        "    segmented_data = segment_content(raw_data)\n",
        "\n",
        "    # Export segmented content to text file\n",
        "    export_segmented_content(segmented_data, output_file)\n",
        "\n",
        "    print(f'Content Chunking completed. Results saved to {output_file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2deab28-e088-4f38-9320-87211c4b35a1",
      "metadata": {
        "id": "d2deab28-e088-4f38-9320-87211c4b35a1"
      },
      "source": [
        "# Chunks to Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b03f46f0-82c4-468e-a879-837383e9d9be",
      "metadata": {
        "id": "b03f46f0-82c4-468e-a879-837383e9d9be"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "compute_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Utilizing computational device: {compute_device}\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embedding_model.to(compute_device)\n",
        "\n",
        "def text_block_iterator(input_file, block_size=1000):\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        text_blocks = []\n",
        "        for line in file:\n",
        "            text_blocks.append(line.strip())\n",
        "            if len(text_blocks) == block_size:\n",
        "                yield text_blocks\n",
        "                text_blocks = []\n",
        "        if text_blocks:\n",
        "            yield text_blocks\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    return [token for token in cleaned_text.split() if token not in STOPWORDS]\n",
        "\n",
        "def build_topic_model(text_iterator, topic_count=10):\n",
        "    word_dict = corpora.Dictionary()\n",
        "    doc_term_matrix = []\n",
        "    for blocks in text_iterator:\n",
        "        processed_blocks = [clean_and_tokenize(block) for block in blocks]\n",
        "        word_dict.add_documents(processed_blocks)\n",
        "        doc_term_matrix.extend([word_dict.doc2bow(text) for text in processed_blocks])\n",
        "    lda_model = LdaModel(corpus=doc_term_matrix, id2word=word_dict, num_topics=topic_count, random_state=42)\n",
        "    return lda_model, word_dict\n",
        "\n",
        "def extract_dominant_topic(text_block, lda_model, word_dict):\n",
        "    bow = word_dict.doc2bow(clean_and_tokenize(text_block))\n",
        "    topic_distribution = lda_model.get_document_topics(bow)\n",
        "    return max(topic_distribution, key=lambda x: x[1])[0] if topic_distribution else None\n",
        "\n",
        "@torch.no_grad()\n",
        "def analyze_and_persist_blocks(text_blocks, lda_model, word_dict, output_file, batch_size=64):\n",
        "    with open(output_file, 'a', encoding='utf-8') as out_file:\n",
        "        for i in tqdm(range(0, len(text_blocks), batch_size), desc=\"Analyzing batches\"):\n",
        "            current_batch = text_blocks[i:i + batch_size]\n",
        "            try:\n",
        "                embeddings = embedding_model.encode(current_batch, convert_to_tensor=True, device=compute_device)\n",
        "                topics = [extract_dominant_topic(block, lda_model, word_dict) for block in current_batch]\n",
        "\n",
        "                for block, embedding, topic in zip(current_batch, embeddings, topics):\n",
        "                    embedding_str = ' '.join(map(str, embedding.cpu().numpy().tolist()))\n",
        "                    out_file.write(f\"{block}\\t{embedding_str}\\t{topic}\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {i//batch_size}: {e}\")\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    input_file = 'segmented_content.txt'\n",
        "    result_file = 'analyzed_text_data.txt'\n",
        "\n",
        "    try:\n",
        "        print(\"Constructing topic model...\")\n",
        "        lda_model, word_dict = build_topic_model(text_block_iterator(input_file))\n",
        "\n",
        "        print(\"Analyzing text blocks and persisting results...\")\n",
        "        for text_blocks in text_block_iterator(input_file):\n",
        "            analyze_and_persist_blocks(text_blocks, lda_model, word_dict, result_file, batch_size=64)\n",
        "            gc.collect()\n",
        "\n",
        "        print(f'Text analysis and embedding generation completed. Results saved to {result_file}')\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e214ef2-fca1-465d-bbc9-5679f7b49ef8",
      "metadata": {
        "id": "9e214ef2-fca1-465d-bbc9-5679f7b49ef8"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "\n",
        "# Ensure necessary NLTK data is available\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "documents=\"segmented_content.txt\"\n",
        "\n",
        "def create_and_save_index(embedding_model, documents, index_variant=\"flat\"):\n",
        "\n",
        "    # Create the FAISS index\n",
        "    index = faiss.IndexFlatL2(embedding_model.shape[1])\n",
        "\n",
        "    # Add embeddings to the index\n",
        "    index.add(embedding_model)\n",
        "\n",
        "    # Save the FAISS index to a file\n",
        "    faiss.write_index(index, f\"cuda_{index_variant.lower()}.index\")\n",
        "\n",
        "    # Create metadata dictionary\n",
        "    document_data = {doc[\"id\"]: {\"title\": doc[\"title\"]} for doc in documents}\n",
        "\n",
        "    # Save metadata to a pickle file\n",
        "    with open(f\"cuda_{index_variant.lower()}_metadata.pkl\", 'wb') as f:\n",
        "        pickle.dump(document_data, f)\n",
        "\n",
        "\n",
        "def load_search_components(index_variant):\n",
        "    vector_index = faiss.read_index(f\"cuda_{index_variant.lower()}.index\")\n",
        "    with open(f\"cuda_{index_variant.lower()}_metadata.pkl\", 'rb') as f:\n",
        "        document_data = pickle.load(f)\n",
        "    return vector_index, document_data\n",
        "\n",
        "def initialize_lexical_index(document_data):\n",
        "    tokenized_docs = [doc[0].split() for doc in document_data]\n",
        "    return BM25Okapi(tokenized_docs)\n",
        "\n",
        "def sanitize_input(text):\n",
        "    # Strip non-alphabetic characters and convert to lowercase\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "\n",
        "def determine_pos(term):\n",
        "    tag = nltk.pos_tag([term])[0][1][0].upper()\n",
        "    pos_mapping = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return pos_mapping.get(tag, wordnet.NOUN)\n",
        "\n",
        "def enrich_query(original_query, synonym_count=3):\n",
        "    additional_terms = []\n",
        "    for term in original_query.split():\n",
        "        term_synonyms = []\n",
        "        for synset in wordnet.synsets(term):\n",
        "            for lemma in synset.lemmas():\n",
        "                if lemma.name() != term and lemma.name() not in term_synonyms:\n",
        "                    term_synonyms.append(lemma.name())\n",
        "                    if len(term_synonyms) == synonym_count:\n",
        "                        break\n",
        "            if len(term_synonyms) == synonym_count:\n",
        "                break\n",
        "        additional_terms.extend(term_synonyms)\n",
        "    return original_query + ' ' + ' '.join(additional_terms)\n",
        "\n",
        "def apply_relevance_feedback(query_embedding, vector_index, document_data, top_k=5, feedback_weight=0.3):\n",
        "    # Initial search\n",
        "    _, top_indices = vector_index.search(query_embedding.reshape(1, -1), top_k)\n",
        "\n",
        "    # Extract top documents\n",
        "    top_documents = [document_data[i][0] for i in top_indices[0]]\n",
        "\n",
        "    # Create TF-IDF representation\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(top_documents)\n",
        "\n",
        "    # Compute centroid\n",
        "    centroid = tfidf_matrix.mean(axis=0)\n",
        "\n",
        "    # Expand query embedding\n",
        "    expanded_embedding = query_embedding + feedback_weight * embedding_model.encode(tfidf_vectorizer.get_feature_names_out()[centroid.argmax()])\n",
        "\n",
        "    return expanded_embedding\n",
        "\n",
        "def perform_hybrid_search(vector_index, lexical_index, document_data, user_query, result_count=5,\n",
        "                          vector_weight=0.5, use_query_enrichment=True, use_relevance_feedback=True):\n",
        "    # Preprocess and optionally enrich the query\n",
        "    clean_query = sanitize_input(user_query)\n",
        "    if use_query_enrichment:\n",
        "        enriched_query = enrich_query(clean_query)\n",
        "    else:\n",
        "        enriched_query = clean_query\n",
        "\n",
        "    # Vector-based search\n",
        "    query_embedding = embedding_model.encode(enriched_query)\n",
        "    if use_relevance_feedback:\n",
        "        query_embedding = apply_relevance_feedback(query_embedding, vector_index, document_data)\n",
        "    vector_distances, vector_indices = vector_index.search(query_embedding.reshape(1, -1), result_count*2)\n",
        "\n",
        "    # Lexical search\n",
        "    lexical_scores = lexical_index.get_scores(enriched_query.split())\n",
        "    lexical_top_indices = np.argsort(lexical_scores)[::-1][:result_count*2]\n",
        "\n",
        "    # Merge results\n",
        "    combined_relevance = {}\n",
        "    for i, idx in enumerate(vector_indices[0]):\n",
        "        combined_relevance[idx] = vector_weight * (1 - vector_distances[0][i])  # Convert distance to similarity\n",
        "\n",
        "    for i, idx in enumerate(lexical_top_indices):\n",
        "        if idx in combined_relevance:\n",
        "            combined_relevance[idx] += (1 - vector_weight) * (lexical_scores[idx] / max(lexical_scores))\n",
        "        else:\n",
        "            combined_relevance[idx] = (1 - vector_weight) * (lexical_scores[idx] / max(lexical_scores))\n",
        "\n",
        "    # Select top results\n",
        "    top_indices = sorted(combined_relevance, key=combined_relevance.get, reverse=True)[:result_count]\n",
        "\n",
        "    search_results = []\n",
        "    for idx in top_indices:\n",
        "        content, category, source = document_data[idx]\n",
        "        search_results.append({\n",
        "            \"content\": content,\n",
        "            \"category\": category,\n",
        "            \"source\": source,\n",
        "            \"relevance\": combined_relevance[idx]\n",
        "        })\n",
        "    return search_results\n",
        "\n",
        "def main():\n",
        "    index_variant = \"FLAT\"  # Alternative: \"IVF\"\n",
        "    vector_index, document_data = load_search_components(index_variant)\n",
        "    lexical_index = initialize_lexical_index(document_data)\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"Enter your search query (or 'exit' to quit): \")\n",
        "        if user_query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        results = perform_hybrid_search(vector_index, lexical_index, document_data, user_query)\n",
        "\n",
        "        print(\"\\nSearch Results:\")\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"{i}. Content: {result['content'][:100]}...\")\n",
        "            print(f\"   Category: {result['category']}\")\n",
        "            print(f\"   Source: {result['source']}\")\n",
        "            print(f\"   Relevance: {result['relevance']:.4f}\")\n",
        "            print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c461cc71",
      "metadata": {
        "id": "c461cc71"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Set your OpenAI API key directly here\n",
        "OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\"\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Initialize SentenceTransformer model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def build_context(search_results: List[Dict], user_query: str) -> str:\n",
        "    \"\"\"Build the context for the LLM from search results, ranking by relevance to the query.\"\"\"\n",
        "    query_embedding = embedding_model.encode(user_query)\n",
        "    result_embeddings = embedding_model.encode([result['chunk'] for result in search_results])\n",
        "\n",
        "    similarities = np.dot(result_embeddings, query_embedding) / (np.linalg.norm(result_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
        "\n",
        "    ranked_results = [result for _, result in sorted(zip(similarities, search_results), key=lambda x: x[0], reverse=True)]\n",
        "\n",
        "    context_text = \"Here are some relevant passages from the CUDA documentation, ordered by relevance:\\n\\n\"\n",
        "    for idx, result in enumerate(ranked_results, 1):\n",
        "        context_text += f\"{idx}. {result['chunk']}\\n\\n\"\n",
        "    return context_text\n",
        "\n",
        "def generate_answer(question: str, search_results: List[Dict]) -> str:\n",
        "    \"\"\"Use GPT to answer the question based on the retrieved and ranked results.\"\"\"\n",
        "    context = build_context(search_results, question)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant that answers questions about CUDA based on the provided context.\n",
        "        Follow these guidelines:\n",
        "        1. Always base your answers on the information provided in the context.\n",
        "        2. If the answer cannot be found in the context, clearly state that you don't have enough information to answer accurately.\n",
        "        3. If the context contains conflicting information, mention this and explain the different viewpoints.\n",
        "        4. Use technical terms correctly and explain them if they're complex.\n",
        "        5. If appropriate, structure your answer with bullet points or numbered lists for clarity.\n",
        "        6. Cite the relevant passage numbers from the context to support your answer.\n",
        "        7. If the user's question is unclear, ask for clarification before attempting to answer.\"\"\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"}\n",
        "    ]\n",
        "\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        max_tokens=1000,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def main():\n",
        "    idx_type = \"FLAT\"  # or \"IVF\"\n",
        "    index, metadata = load_search_components(idx_type)  # Assuming these functions are defined elsewhere\n",
        "    lex_index = initialize_lexical_index(metadata)  # Assuming this function is defined elsewhere\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter your question about CUDA (or 'quit' to exit): \")\n",
        "        if query.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        search_results = hybrid_search(index, lex_index, metadata, query, k=5, alpha=0.5, use_query_expansion=True, use_prf=True)\n",
        "\n",
        "        print(\"\\nRetrieved Passages:\")\n",
        "        for idx, result in enumerate(search_results, 1):\n",
        "            print(f\"{idx}. {result['chunk'][:100]}...\")\n",
        "\n",
        "        answer = generate_answer(query, search_results)\n",
        "        print(\"\\nAnswer:\")\n",
        "        print(answer)\n",
        "        print(\"---\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ea7fb4-0747-4163-9864-e5ca8cb2d815",
      "metadata": {
        "id": "f1ea7fb4-0747-4163-9864-e5ca8cb2d815"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from ChatGPT import answer\n",
        "from Vector_Retrieval_Reranking import hybrid_search\n",
        "def main():\n",
        "    idx_type = \"FLAT\"  # or \"IVF\"\n",
        "    index, metadata = load_search_components(idx_type)\n",
        "    lex_index = initialize_lexical_index(metadata)\n",
        "\n",
        "    def inference(query):\n",
        "        nonlocal index, lex_index, metadata\n",
        "        results = hybrid_search(index, lex_index, metadata, query, k=5, alpha=0.5, use_query_expansion=True, use_prf=True)\n",
        "        answer = generate_answer(query, results)\n",
        "        return answer\n",
        "\n",
        "    iface = gr.Interface(\n",
        "        fn=inference,\n",
        "        inputs=\"text\",\n",
        "        outputs=\"text\",\n",
        "        title=\"CUDA Documentation Assistant\",\n",
        "        description=\"Ask a question about CUDA documentation.\",\n",
        "        theme=\"huggingface\",\n",
        "        examples=[[\"How to use CUDA with Python?\"]],\n",
        "    )\n",
        "    iface.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IELMvVXgo67f",
      "metadata": {
        "id": "IELMvVXgo67f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
